syntax = "proto2";

import "store/common/common-proto.proto";
import "store/pequinstore/pequin-proto.proto";

package pequinstore.proto;

/////////////////////////// Query Exec ///////////////////////////
message Query {
   required uint64 query_seq_num = 1;
   required uint64 client_id = 2; //query unique identifier = (query_seq_num, client_id) uniquely defines a corret clients query. A byz one can have duplicates, but thats only hurting its own progress                               
   // Unless: Dependency on another TX which has a query that is invalid... (will abort by default) --> but then TX should not have prepared --> TxID still uniquely defines which query is used.
   // For now: Treat Query just as a read, and assume read set will be included in tx (instead of cached) -- all worries about attack are complications due to caching.
   required bytes query_cmd = 3;      //SQL statement as string --> parse at Replica
                                  //Alternatively: Already parsed into AST at client --> send AST (What format?)
   required TimestampMessage timestamp = 4; //Timestamp at which to read query.
   optional uint64 query_manager = 5;
   //TODO: Needs to include involved groups?
   

}

message QueryRequest {
  required uint64 req_id = 1;
  oneof query_oneof {
    Query query = 2;
    SignedMessage signed_query = 3; //Need to sign query to a) provide access control (could use Macs instead), b) prove that (query_id, client_id) was proposed only by client with client_id
    //private channels from client to replicas should suffice?
  }
  optional bool optimistic_txid = 4;
  optional bool retry = 5;
}

message Result {
  required uint64 query_seq_num = 1;
  required uint64 client_id = 2;
  //include result: this may be anything: e.g. an integer, a table (list of values). TBD how to encode
  required bytes query_result = 3;
  required bytes query_result_hash = 4; //Hash of (QueryID, Read Set Merkle Root) --> uniquely identifies a queries read set.
  required uint64 replica_id = 5;
}

message QueryResult {
    //identify query
    required uint64 req_id = 1;
    oneof result_oneof {
      Result result = 2;
      SignedMessage signed_result = 3;
    }
}



////////////////////////// Query Synchronization ///////////////////////////

//replica receives Query and replies with local snapshot of relevant state for ExecQuery
message LocalSnapshot {
  //identify query
  required uint64 query_seq_num = 1;
  required uint64 client_id = 2;
  //list of txn that materialize local exec snapshot
  repeated bytes local_txns_committed = 3;
  repeated bytes local_txns_prepared = 4;
  required uint64 replica_id = 5; 
}
message SyncReply {
  required uint64 req_id = 1;
  oneof snapshot_oneof {
    LocalSnapshot local_ss = 2;
    SignedMessage signed_local_ss = 3;
  }
}

message ReplicaList {
  repeated uint64 replicas = 1; //list of replicas
}

//can use this instead of map <string, ReplicaList>
message TxnData {
  required bytes txn = 1;
  repeated uint64 replicas = 2; //list of replicas that have txn.
  required bool committed = 3 ; // true if committed, false if prepared
}

message MergedSnapshot {
   //identify query
   required uint64 query_seq_num= 1;
   required uint64 client_id = 2;
   //include snapshot proposal. 
      //repeated TxnData cp_txns = 3;
   map<string, ReplicaList> merged_txns_committed = 3; //map from txn digest to replicas that have txn.
   map<string, ReplicaList> merged_txns_prepared = 4; //dont think one needs to distinguish at this point.
   optional bytes query_digest = 5;
}

//client sends proposed merged snapshot. Replicas use it to ExecQuery and send back QueryReply
message SyncClientProposal {
 required uint64 req_id = 1;
 oneof snapshot_oneof {
   MergedSnapshot merged_ss = 2;
   SignedMessage signed_merged_ss = 3;
 }
 required bool reply = 4; //designate whether this replica should execute and reply, or just sync, execute, and cache read set.
 optional bool query_managager = 5; // designate this shard as tx manager to coordinate execution and to reply.
 optional bool retry = 6;
}

message RequestMissingTxns {
   repeated bytes missing_txn = 1; //list of txn digests;
   required uint64 replica_id = 2;
}

message TxnProof {
  optional Transaction txn = 1;
  optional CommittedProof commit_proof = 2;
}

message SupplyMissingTxnsMessage {
   map<string, TxnProof> txns = 1; //map from txn digest to Transaction object (+ proof)
   optional uint64 replica_id = 2;
}

message SupplyMissingTxns {
  oneof supply_missing_txn {
    SupplyMissingTxnsMessage supply_txn = 1;
    SignedMessage signed_supply_txn = 2;
  }
}

////////////////////////// Modified Concurrency Control ///////////////////////////
//TODO: Redefine Transaction to include also query information.
message TransactionQuery {
  //Same as Transaction, but additional fields for query information
  required Transaction txn = 1;
  repeated QueryResult query_result= 2;
}

////////////////////////// Checkpointing ///////////////////////////
message LocalCheckpointSnapshot {
  required uint64 checkpoint_id = 1;
  repeated bytes transactions = 2; //list of txn that form current checkpoint
  required uint64 replica_id = 3;
  required uint64 checkpoint_view = 4;
  //TODO: needs to be signed by replica.
}

message CheckpointVote {
  //send local snapshot from replica to
  required uint64 checkpoint_view = 1; //duplicate just for easier access?
  oneof local_snapshot {
    LocalCheckpointSnapshot local_cp_ss = 2;
    SignedMessage signed_local_cp_ss = 3;
  }
}

message CheckpointVotes {
  repeated CheckpointVote votes = 1;
}

message MergedCheckpointSnapshot {
  map<string, ReplicaList> relevant_txns = 1; //map from txn digest to replicas that have txn. --> technically just need set of votes, since replicas need to reassemble themselves anyways.
  oneof merged_proof {
    CheckpointVotes cp_votes = 2;
    SignedMessages signed_cp_votes = 3;
  }
}

message CheckpointProposal {
   //send local snapshot from replica to
   required uint64 checkpoint_view = 1;
   required uint64 checkpoint_id = 2;
   required bytes cp_hash = 3; //send hash of MergedCheckpointSnapshot.
   optional MergedCheckpointSnapshot local_cp_ss = 4;  //first leader needs to include it; replicas will only accept hash if they have set of merged snapshot checkpoints.
   //TODO: Needs to be signed by leader of view.
}

//replica echos CheckpointProposal (if first to be received in view, and no higher view received)
message CheckpointAccept {
  //simply an echo (possibly signed) of proposal.
  required uint64 replica_id = 1;
  oneof accept_oneof {
    CheckpointProposal cp_proposal = 2;   //Remove local_cp_ss before sending.
    SignedMessage signed_cp_proposal = 3;
  }
}

//leader confirms checkpoint. replicas accept if valid.
//Does not need to be signed -- anybody can issue a confirm since the existance of it guarantees that nothing else can be confirmed.
message CheckpointConfirm {
  required uint64 checkpoint_view = 1;
  required uint64 checkpoint_id = 2;
  required bytes cp_hash = 3; //send hash of MergedCheckpointSnapshot.
  optional MergedCheckpointSnapshot local_cp_ss = 4;
  optional Signatures cp_votes = 5; //Set of CheckpointAccept messages (or rather, sigs belonging to it)
}

// Checkpoint view change messages.
message RequestNewCPLeader {
   // Just need mechanism to complain --> If f+1 complaints received, or timed out, increment view
   //If timeout to receive Checkpoint proposal --> send new votes to next leader
   //If timeout to receive Checkpoint Confirm --> send checkpoint accept to next leader. (also include)

}